# phyddle

A pipeline-based toolkit for fiddling around with Phylogenetic Deep Learning.

## Brief guide

A standard phyddle analysis performs the following tasks for you:
- **Pipeline configuration**, applies analysis settings provided through a config file and/or command line arguments
- **Model configuration**, constructs a base simulating model to be *Simulated* (states, events, rates)
- **Simulating**, simulates a large training dataset under the model to be *Formatted* (parallelized, partly compressed)
- **Formatting**, encodes the raw simulated data into tensor format for *Learning* (parallelized, compressed)
- **Learning**, shuffles and splits training data, builds a network, then trains and saves the network with the data for *Prediction*
- **Predicting**, estimates model parameters for a new dataset with the trained network
- **Plotting**, generates figures that summarize the training data (*Formatting*), the network and its training (*Learning*), and any new predictions (*Predicting*)

To run a phyddle analysis enter the `code` directory:
```
cd ~/projects/phyddle/code
```

Then create and run a pipeline under the settings you've specified in `my_config.py`:
```
./run_pipeline.sh --cfg my_config
```

This will run a phyddle analysis for a simple 3-region GeoSSE model with just 500 training examples. In practice, you'll want to generate a larger training dataset with anywhere from 10k to 1M examples, depending on the model.

To add new examples to your training set
```
# simulate new training examples and store in raw_data/my_project
./run_simulate.sh --cfg my_config --start_idx 500 --end_idx 15000

# encode all raw_data examples as tensors in tensor_data/my_project
./run_format.sh --cfg my_config --start_idx 0 --end_idx 15000

# train network with tensor_data, but override batch size, then store in network/my_project
./run_learn.sh --cfg my_config --batch_size 256

# make prediction with example dataset
./run_predict.sh --cfg my_config

# generate figures and store in plot
./run_plot.sh --cfg my_config
```

Pipeline options are applied to all pipeline stages. See the full list of currently supported options with
```
./run_pipeline.sh --help
```

## Features

Current features:
- trained network generates parameter estimates and coverage-calibrated prediction intervals (CPIs) for input datasets
- provides several state-dependent birth-death model types and variants (more to come)
- parallelized simulating, formatting, and learning
- encoding of phylogenetic-state tensor from serial and extant-only input with multiple states (CBLV+S and CDV+S extensions)
- encoding of auxiliary data tensor from automatically computed summary statistics and "known" parameter (e.g. sampling rate)
- HDF5 with gzip compression for tensor data
- shuffles and splits input tensors into training, test, validation, and calibration datasets for supervised learning
- builds network with convolution, pooling, and dense layers that match input tensors
- trains network and saves history
- automatic figure generation with Matplotlib

## Longer guide

The repository has five main directories:
- `code` contains scripts to generate and process data
- `raw_data` contains raw data generated by simulation
- `tensor_data` contains data formatted into tensors for training networks
- `network` contains trained networks and diagnostics
- `plot` contains figures of training and validation procedures

In general, the pipeline assumes that the user supplies runs scripts in `code` using a consistent *project name* (e.g. `my_project`) to coordinate the analysis across the `raw_data`, `tensor_data`, `network`, and `plot` directories.


